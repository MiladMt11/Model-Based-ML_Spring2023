{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a99528",
   "metadata": {},
   "source": [
    "# 42186 Model-based machine learning\n",
    "# Modelling traffic accident injury severity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b5203",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf694e91",
   "metadata": {},
   "source": [
    "First we read all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343397e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import make_scorer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "from pyro.infer import MCMC, NUTS, HMC, SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "import os\n",
    "from torch import nn\n",
    "from pyro.nn import PyroModule\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef816d",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2057a",
   "metadata": {},
   "source": [
    "Then we read our data, select the features that we want to use and do some preproceesing in order to get date information (hour, day of week and so on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3f25ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/frida/OneDrive - Danmarks Tekniske Universitet/Model based machine learning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3b7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/Users/frida/OneDrive - Danmarks Tekniske Universitet/Model based machine learning/ACCIDENT/'\n",
    "\n",
    "df_event = pd.read_csv(path + \"ACCIDENT_EVENT.csv\")\n",
    "df_loc = pd.read_csv(path + \"ACCIDENT_LOCATION.csv\")\n",
    "df_acc = pd.read_csv(path + \"ACCIDENT.csv\",low_memory=False)\n",
    "df_veh = pd.read_csv(path + \"VEHICLE.csv\",low_memory=False)\n",
    "df_person = pd.read_csv(path + \"PERSON.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52670158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCIDENT_TYPE</th>\n",
       "      <th>LIGHT_CONDITION</th>\n",
       "      <th>ROAD_GEOMETRY</th>\n",
       "      <th>SPEED_ZONE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>day_of_week_sin</th>\n",
       "      <th>day_of_week_cos</th>\n",
       "      <th>MONTH_sin</th>\n",
       "      <th>MONTH_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>12</td>\n",
       "      <td>-7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>19</td>\n",
       "      <td>-7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>11</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2006</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326633</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>18</td>\n",
       "      <td>7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326634</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326635</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326636</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>01</td>\n",
       "      <td>4.338837e-01</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326637</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>01</td>\n",
       "      <td>4.338837e-01</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326638 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ACCIDENT_TYPE  LIGHT_CONDITION  ROAD_GEOMETRY  SPEED_ZONE  YEAR HOUR  \\\n",
       "0                   1                1              1           4  2006   12   \n",
       "1                   1                1              2           5  2006   19   \n",
       "2                   7                1              5           9  2006   12   \n",
       "3                   1                1              2           7  2006   11   \n",
       "4                   1                1              5           3  2006   10   \n",
       "...               ...              ...            ...         ...   ...  ...   \n",
       "326633              1                1              1           4  2020   18   \n",
       "326634              6                1              5           7  2020   12   \n",
       "326635              6                1              5           7  2020   12   \n",
       "326636              4                3              5           7  2020   01   \n",
       "326637              4                3              5           7  2020   01   \n",
       "\n",
       "        day_of_week_sin  day_of_week_cos  MONTH_sin  MONTH_cos  \n",
       "0         -7.818315e-01         0.623490        0.5   0.866025  \n",
       "1         -7.818315e-01         0.623490        0.5   0.866025  \n",
       "2         -2.449294e-16         1.000000        0.5   0.866025  \n",
       "3         -2.449294e-16         1.000000        0.5   0.866025  \n",
       "4         -2.449294e-16         1.000000        0.5   0.866025  \n",
       "...                 ...              ...        ...        ...  \n",
       "326633     7.818315e-01         0.623490       -0.5   0.866025  \n",
       "326634     7.818315e-01         0.623490       -0.5   0.866025  \n",
       "326635     7.818315e-01         0.623490       -0.5   0.866025  \n",
       "326636     4.338837e-01        -0.900969       -0.5   0.866025  \n",
       "326637     4.338837e-01        -0.900969       -0.5   0.866025  \n",
       "\n",
       "[326638 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_cols = df_acc.columns.intersection(df_loc.columns)\n",
    "inter_cols = [col for col in inter_cols]\n",
    "data = pd.merge(df_acc, df_loc, on=inter_cols, how='inner')\n",
    "\n",
    "inter_cols = data.columns.intersection(df_event.columns)\n",
    "inter_cols = [col for col in inter_cols]\n",
    "data = pd.merge(data, df_event, on=inter_cols, how='inner')\n",
    "y = data['SEVERITY']\n",
    "\n",
    "# Selecting the relevant features \n",
    "data = data[['ACCIDENTDATE', 'ACCIDENTTIME', 'ACCIDENT_TYPE', 'DAY_OF_WEEK', 'LIGHT_CONDITION', \n",
    "             'ROAD_GEOMETRY', 'SPEED_ZONE']]\n",
    "data['ACCIDENTDATE'] = pd.to_datetime(data['ACCIDENTDATE'], format=\"%d/%m/%Y\")\n",
    "#data['ACCIDENTTIME'] = pd.to_datetime(data['ACCIDENTTIME'], format=\"%H:%M:%S\")\n",
    "data['MONTH'] = data['ACCIDENTDATE'].dt.month\n",
    "data['YEAR'] = data['ACCIDENTDATE'].dt.year\n",
    "data['HOUR'] = data['ACCIDENTTIME'].apply(lambda x: x[0:2])\n",
    "data['LIGHT_CONDITION'] = data['LIGHT_CONDITION'].replace(9,7,regex=True)\n",
    "data['DAY_OF_WEEK'] = data['DAY_OF_WEEK'].replace(0,1,regex=True)\n",
    "# Making time features periodic\n",
    "data['day_of_week_sin'] = np.sin(data['DAY_OF_WEEK'] * (2 * np.pi / 7))\n",
    "data['day_of_week_cos'] = np.cos(data['DAY_OF_WEEK'] * (2 * np.pi / 7))\n",
    "data['MONTH_sin'] = np.sin(data['MONTH']* (2 * np.pi / 12))\n",
    "data['MONTH_cos'] = np.cos(data['MONTH']* (2 * np.pi / 12))\n",
    "\n",
    "# Dropping features that have been modifed\n",
    "data = data.drop(['ACCIDENTDATE', 'ACCIDENTTIME', 'MONTH', 'DAY_OF_WEEK'], axis = 1)\n",
    "\n",
    "# Making SPEED_ZONE categorical\n",
    "rep = {30:1 , 40:2, 50: 3, 60:4, 70:5, 75:6, 80:7, 90: 8, 100:9, 110:10, 777:11, 888:12, 999:13}\n",
    "data['SPEED_ZONE'] = data['SPEED_ZONE'].replace(rep)\n",
    "\n",
    "X = data\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20680525",
   "metadata": {},
   "source": [
    "We will now use the Sclearn function SelectKBest in order to find the most important features to predict severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d264b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Specs     Score\n",
      "0    ACCIDENT_TYPE  0.025207\n",
      "2    ROAD_GEOMETRY  0.024323\n",
      "1  LIGHT_CONDITION  0.020074\n",
      "3       SPEED_ZONE  0.018345\n",
      "5             HOUR  0.008624\n",
      "4             YEAR  0.007472\n",
      "7  day_of_week_cos  0.006617\n",
      "6  day_of_week_sin  0.004802\n",
      "9        MONTH_cos  0.004348\n",
      "8        MONTH_sin  0.001932\n"
     ]
    }
   ],
   "source": [
    "bestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2249b",
   "metadata": {},
   "source": [
    "We choose the three most important features ROAD_GEOMETRY, ACCIDENT_TYPE, LIGHT_CONDITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "aa553c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving number of unique entries in the three most important features\n",
    "n_acc = len(data_.ACCIDENT_TYPE.unique())\n",
    "n_light = len(data_.LIGHT_CONDITION.unique())\n",
    "n_road = len(data_.ROAD_GEOMETRY.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ebd42",
   "metadata": {},
   "source": [
    "Pivot tables for the chosen features vs. severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ffbabdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ROAD_GEOMETRY</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEVERITY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012783</td>\n",
       "      <td>0.015909</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.015688</td>\n",
       "      <td>0.031156</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.312064</td>\n",
       "      <td>0.342381</td>\n",
       "      <td>0.305376</td>\n",
       "      <td>0.335280</td>\n",
       "      <td>0.386767</td>\n",
       "      <td>0.413462</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.362193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.675138</td>\n",
       "      <td>0.641696</td>\n",
       "      <td>0.686022</td>\n",
       "      <td>0.649032</td>\n",
       "      <td>0.582066</td>\n",
       "      <td>0.567308</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.629149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ROAD_GEOMETRY         1         2         3         4         5         6  \\\n",
       "SEVERITY                                                                    \n",
       "1              0.012783  0.015909  0.008602  0.015688  0.031156  0.019231   \n",
       "2              0.312064  0.342381  0.305376  0.335280  0.386767  0.413462   \n",
       "3              0.675138  0.641696  0.686022  0.649032  0.582066  0.567308   \n",
       "4              0.000015  0.000014       NaN       NaN  0.000011       NaN   \n",
       "\n",
       "ROAD_GEOMETRY    7      8         9  \n",
       "SEVERITY                             \n",
       "1              NaN    NaN  0.008658  \n",
       "2              0.5  0.125  0.362193  \n",
       "3              0.5  0.875  0.629149  \n",
       "4              NaN    NaN       NaN  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = data_.groupby(['SEVERITY','ROAD_GEOMETRY']).size().unstack()\n",
    "ls.div(ls.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b23d6e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ACCIDENT_TYPE</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEVERITY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.034589</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.044013</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>0.012464</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>0.011429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.309581</td>\n",
       "      <td>0.413287</td>\n",
       "      <td>0.353924</td>\n",
       "      <td>0.463689</td>\n",
       "      <td>0.364421</td>\n",
       "      <td>0.394203</td>\n",
       "      <td>0.400499</td>\n",
       "      <td>0.373021</td>\n",
       "      <td>0.337143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.552076</td>\n",
       "      <td>0.628743</td>\n",
       "      <td>0.492298</td>\n",
       "      <td>0.620546</td>\n",
       "      <td>0.585789</td>\n",
       "      <td>0.587038</td>\n",
       "      <td>0.618987</td>\n",
       "      <td>0.651429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ACCIDENT_TYPE         1         2         3         4         5         6  \\\n",
       "SEVERITY                                                                    \n",
       "1              0.016114  0.034589  0.017334  0.044013  0.015033  0.019940   \n",
       "2              0.309581  0.413287  0.353924  0.463689  0.364421  0.394203   \n",
       "3              0.674300  0.552076  0.628743  0.492298  0.620546  0.585789   \n",
       "4              0.000005  0.000048       NaN       NaN       NaN  0.000069   \n",
       "\n",
       "ACCIDENT_TYPE         7         8         9  \n",
       "SEVERITY                                     \n",
       "1              0.012464  0.007917  0.011429  \n",
       "2              0.400499  0.373021  0.337143  \n",
       "3              0.587038  0.618987  0.651429  \n",
       "4                   NaN  0.000075       NaN  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = data_.groupby(['SEVERITY','ACCIDENT_TYPE']).size().unstack()\n",
    "ls.div(ls.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dbcf0c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>LIGHT_CONDITION</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEVERITY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019888</td>\n",
       "      <td>0.018194</td>\n",
       "      <td>0.025267</td>\n",
       "      <td>0.039766</td>\n",
       "      <td>0.068714</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.008672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.344234</td>\n",
       "      <td>0.351983</td>\n",
       "      <td>0.412896</td>\n",
       "      <td>0.410526</td>\n",
       "      <td>0.457031</td>\n",
       "      <td>0.286720</td>\n",
       "      <td>0.181711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.635869</td>\n",
       "      <td>0.629824</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>0.549708</td>\n",
       "      <td>0.474255</td>\n",
       "      <td>0.698189</td>\n",
       "      <td>0.809618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "LIGHT_CONDITION         1         2         3         4         5         6  \\\n",
       "SEVERITY                                                                      \n",
       "1                0.019888  0.018194  0.025267  0.039766  0.068714  0.015091   \n",
       "2                0.344234  0.351983  0.412896  0.410526  0.457031  0.286720   \n",
       "3                0.635869  0.629824  0.561800  0.549708  0.474255  0.698189   \n",
       "4                0.000009       NaN  0.000037       NaN       NaN       NaN   \n",
       "\n",
       "LIGHT_CONDITION         9  \n",
       "SEVERITY                   \n",
       "1                0.008672  \n",
       "2                0.181711  \n",
       "3                0.809618  \n",
       "4                     NaN  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = data_.groupby(['SEVERITY','LIGHT_CONDITION']).size().unstack()\n",
    "ls.div(ls.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9889a",
   "metadata": {},
   "source": [
    "For all the pivot tables we see a very clear distinction between the severity categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46138c",
   "metadata": {},
   "source": [
    "Next, we extract observations from out data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "df3598ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "numobs = 5000\n",
    "X = data[['ACCIDENT_TYPE', 'LIGHT_CONDITION', 'ROAD_GEOMETRY']]\n",
    "X = X[:numobs]\n",
    "y = data_['SEVERITY']\n",
    "y = y[:numobs]\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "ind = np.arange(0,numobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f78efb",
   "metadata": {},
   "source": [
    "We then prepare our test and training sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "b942668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 3300\n",
      "num test: 1700\n"
     ]
    }
   ],
   "source": [
    "train_perc = 0.66 # percentage of training data\n",
    "\n",
    "# Large data set\n",
    "split_point = int(train_perc*len(y))\n",
    "perm = np.random.permutation(len(y))\n",
    "ix_train = perm[:split_point]\n",
    "ix_test = perm[split_point:]\n",
    "X_train = X[ix_train,:]\n",
    "X_test = X[ix_test,:]\n",
    "ind_train = ind[ix_train]\n",
    "ind_test = ind[ix_test]\n",
    "y_train = y[ix_train]\n",
    "y_test = y[ix_test]\n",
    "print(\"num train: %d\" % len(y_train))\n",
    "print(\"num test: %d\" % len(y_test))\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "ind_train = torch.tensor(ind_train).long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993957d",
   "metadata": {},
   "source": [
    "We then define our hierarchical model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "c2a67214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_model(X, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu, obs=None):\n",
    " \n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    mu_c = pyro.sample(\"mu_c\", dist.Normal(torch.zeros(n_cat), \n",
    "                                                       lambda_val*torch.ones(n_cat)).to_event())# Prior for the bias mean\n",
    "        \n",
    "\n",
    "    sigma_c  = pyro.sample(\"sigma_c\",  dist.HalfCauchy(tau*torch.ones(n_cat)).to_event()) # Prior for the bias standard deviation\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(input_dim, n_cat), \n",
    "                                            nu*torch.ones(input_dim, n_cat)).to_event()) # Priors for the regression coefficents\n",
    "\n",
    "    with pyro.plate(\"acc\", n_acc):\n",
    "        with pyro.plate(\"light\", n_light):\n",
    "            with pyro.plate(\"road\", n_road):\n",
    "                alpha = pyro.sample(\"alpha\", dist.Normal(mu_c, sigma_c).to_event(1)) # Draw the individual parameter for each individual\n",
    "\n",
    "    with pyro.plate(\"data\", X.shape[0]):\n",
    "        acc = np.array(X[:,0])\n",
    "        light = np.array(X[:,1])\n",
    "        road = np.array(X[:,2])\n",
    "        logits = alpha[acc-1,light-1,road-1] + X.matmul(beta)\n",
    "        y = pyro.sample(\"y\", dist.Categorical(logits=logits), obs=obs) # If you use logits you don't need to do sigmoid\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463c4d4",
   "metadata": {},
   "source": [
    "Preparing our model. These are simply guesses for the prior values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "e641fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat = 4 #number of severity categories\n",
    "indx = numobs\n",
    "tau = 10\n",
    "lambda_val= 10\n",
    "nu = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e837fe63",
   "metadata": {},
   "source": [
    "Training our model using gradient step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "0425b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 40593.3\n",
      "[500] ELBO: 5232.1\n",
      "[1000] ELBO: 3056.0\n",
      "[1500] ELBO: 2856.4\n",
      "[2000] ELBO: 2808.2\n",
      "[2500] ELBO: 2802.0\n",
      "[3000] ELBO: 2788.3\n",
      "[3500] ELBO: 2777.0\n",
      "[4000] ELBO: 2778.5\n",
      "[4500] ELBO: 2768.8\n",
      "[5000] ELBO: 2763.4\n",
      "[5500] ELBO: 2759.9\n",
      "[6000] ELBO: 2752.4\n",
      "[6500] ELBO: 2756.7\n",
      "[7000] ELBO: 2742.6\n",
      "[7500] ELBO: 2761.6\n",
      "[8000] ELBO: 2818.6\n",
      "[8500] ELBO: 2746.1\n",
      "[9000] ELBO: 2740.8\n",
      "[9500] ELBO: 2741.2\n",
      "[10000] ELBO: 2742.9\n",
      "[10500] ELBO: 2739.0\n",
      "[11000] ELBO: 2737.6\n",
      "[11500] ELBO: 2729.4\n",
      "CPU times: user 3min 16s, sys: 4.72 s, total: 3min 20s\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Train the herarical model without the ANN on the X*beta part \n",
    "# Define guide function\n",
    "guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 12000\n",
    " \n",
    "    \n",
    "# Setup the optimizer\n",
    "adam_params = {\"lr\": 0.005}\n",
    "optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=3)\n",
    "svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu , y_train-1)\n",
    "    if step % 500 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "e1c83c99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                        return_sites=(\"beta\", \"alpha\", \"mu_c\", \"sigma_c\"))\n",
    "samples = predictive(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu , y_train-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18fd92",
   "metadata": {},
   "source": [
    "We then extract expected values of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "839c40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b06ed",
   "metadata": {},
   "source": [
    "and make predictions for test set and compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "14cea4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [3 3 3 ... 3 3 3]\n",
      "true values: [2 2 2 ... 2 2 2]\n",
      "Accuracy: 0.5658823529411765\n"
     ]
    }
   ],
   "source": [
    "acc = X_test[:,0]\n",
    "light = X_test[:,1]\n",
    "road =  X_test[:,2]\n",
    "y_hat = alpha_hat[acc-1,light-1,road-1,:] + np.squeeze(np.dot(X_test, beta_hat[0]))\n",
    "y_hat = np.argmax(y_hat, axis=1) + 1\n",
    "print(\"predictions:\", y_hat)\n",
    "print(\"true values:\", y_test)\n",
    "\n",
    "# evaluate prediction accuracy\n",
    "print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200810f6",
   "metadata": {},
   "source": [
    "We obtain an accuracy on 56.5 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c52e5",
   "metadata": {},
   "source": [
    "# Tuning priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde7111",
   "metadata": {},
   "source": [
    "In order to optimize we tune the priors by computing the accuracy for all combinations of the ranges stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "d2f3dfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for l= 8 t= 8 nu= 6 acc =  0.558235294117647\n",
      "acc for l= 8 t= 8 nu= 8 acc =  0.5623529411764706\n",
      "acc for l= 8 t= 8 nu= 10 acc =  0.5623529411764706\n",
      "acc for l= 8 t= 10 nu= 6 acc =  0.5605882352941176\n",
      "acc for l= 8 t= 10 nu= 8 acc =  0.5517647058823529\n",
      "acc for l= 8 t= 10 nu= 10 acc =  0.5658823529411765\n",
      "acc for l= 8 t= 12 nu= 6 acc =  0.5594117647058824\n",
      "acc for l= 8 t= 12 nu= 8 acc =  0.5517647058823529\n",
      "acc for l= 8 t= 12 nu= 10 acc =  0.5617647058823529\n",
      "acc for l= 10 t= 8 nu= 6 acc =  0.5629411764705883\n",
      "acc for l= 10 t= 8 nu= 8 acc =  0.5529411764705883\n",
      "acc for l= 10 t= 8 nu= 10 acc =  0.5670588235294117\n",
      "acc for l= 10 t= 10 nu= 6 acc =  0.5641176470588235\n",
      "acc for l= 10 t= 10 nu= 8 acc =  0.5505882352941176\n",
      "acc for l= 10 t= 10 nu= 10 acc =  0.5617647058823529\n",
      "acc for l= 10 t= 12 nu= 6 acc =  0.5605882352941176\n",
      "acc for l= 10 t= 12 nu= 8 acc =  0.5641176470588235\n",
      "acc for l= 10 t= 12 nu= 10 acc =  0.5535294117647059\n",
      "acc for l= 12 t= 8 nu= 6 acc =  0.55\n",
      "acc for l= 12 t= 8 nu= 8 acc =  0.56\n",
      "acc for l= 12 t= 8 nu= 10 acc =  0.5635294117647058\n",
      "acc for l= 12 t= 10 nu= 6 acc =  0.5635294117647058\n",
      "acc for l= 12 t= 10 nu= 8 acc =  0.558235294117647\n",
      "acc for l= 12 t= 10 nu= 10 acc =  0.5670588235294117\n",
      "acc for l= 12 t= 12 nu= 6 acc =  0.558235294117647\n",
      "acc for l= 12 t= 12 nu= 8 acc =  0.5535294117647059\n",
      "acc for l= 12 t= 12 nu= 10 acc =  0.5635294117647058\n"
     ]
    }
   ],
   "source": [
    "# Ranges\n",
    "l_v = [8,10,12]\n",
    "t_v = [8,10,12]\n",
    "n_c = [6,8,10]\n",
    "\n",
    "acc_mat = np.zeros((len(l_v),len(t_v),len(n_c)))\n",
    "l=0\n",
    "for lambda_val in l_v:\n",
    "    t = 0\n",
    "    for tau in t_v:\n",
    "        b=0\n",
    "        for nu in n_c:\n",
    "            \n",
    "            # Define guide function\n",
    "            guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "            # Reset parameter values\n",
    "            pyro.clear_param_store()\n",
    "\n",
    "            # Define the number of optimization steps\n",
    "            n_steps = 2500\n",
    "\n",
    "            # Setup the optimizer\n",
    "            adam_params = {\"lr\": 0.005}\n",
    "            optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "            # Setup the inference algorithm\n",
    "            elbo = Trace_ELBO(num_particles=3)\n",
    "            svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "            # Do gradient steps\n",
    "            for step in range(n_steps):\n",
    "                elbo = svi.step(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu, y_train-1)\n",
    "                \n",
    "            predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                                    return_sites=(\"beta\", \"alpha\", \"alpha_mu\", \"alpha_sigma\"))\n",
    "            samples = predictive(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu, y_train-1)\n",
    "            \n",
    "            # extract expected values of the parameters\n",
    "            alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "            beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()\n",
    "\n",
    "            # make predictions for test set\n",
    "            acc = X_test[:,0]\n",
    "            light = X_test[:,1]\n",
    "            road =  X_test[:,2]\n",
    "            y_hat = alpha_hat[acc-1,light-1,road-1,:] + np.squeeze(np.dot(X_test, beta_hat[0]))\n",
    "            y_hat = np.argmax(y_hat, axis=1) + 1\n",
    "\n",
    "            # evaluate prediction accuracy\n",
    "            acc_mat[l,t,b] = 1.0*np.sum(y_hat == y_test) / len(y_test)\n",
    "            print(\"acc for l=\",lambda_val, \"t=\",tau, \"nu=\",nu, \"acc = \", acc_mat[l,t,b])\n",
    "            \n",
    "            b+=1\n",
    "        t+=1\n",
    "    l+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "2035008e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5670588235294117"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(acc_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871eca6",
   "metadata": {},
   "source": [
    "This corresponds to lambda = 12 tau = 10 nu = 10 with an accuracy of 56.7%. Using this information we try to train the model for 12000 steps using these priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "ccea8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 32589.4\n",
      "[500] ELBO: 7034.4\n",
      "[1000] ELBO: 3801.8\n",
      "[1500] ELBO: 3225.0\n",
      "[2000] ELBO: 3007.6\n",
      "[2500] ELBO: 2953.1\n",
      "[3000] ELBO: 2880.7\n",
      "[3500] ELBO: 2857.7\n",
      "[4000] ELBO: 2841.7\n",
      "[4500] ELBO: 2817.0\n",
      "[5000] ELBO: 2819.6\n",
      "[5500] ELBO: 2811.2\n",
      "[6000] ELBO: 2812.7\n",
      "[6500] ELBO: 2793.1\n",
      "[7000] ELBO: 2790.4\n",
      "[7500] ELBO: 2788.6\n",
      "[8000] ELBO: 2791.1\n",
      "[8500] ELBO: 2794.0\n",
      "[9000] ELBO: 2782.9\n",
      "[9500] ELBO: 2772.8\n",
      "[10000] ELBO: 2783.7\n",
      "[10500] ELBO: 2769.8\n",
      "[11000] ELBO: 2770.6\n",
      "[11500] ELBO: 2768.0\n",
      "CPU times: user 3min 22s, sys: 4.95 s, total: 3min 27s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Train the herarical model without the ANN on the X*beta part \n",
    "# Define guide function\n",
    "guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 12000\n",
    " \n",
    "    \n",
    "# Setup the optimizer\n",
    "adam_params = {\"lr\": 0.005}\n",
    "optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=3)\n",
    "svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(X_train, n_cat, n_acc, n_light, n_road, 12, 10, 10, y_train-1)\n",
    "    if step % 500 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "1ac63338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [3 3 2 ... 3 3 3]\n",
      "true values: [3 2 2 ... 2 2 3]\n",
      "Accuracy: 0.5747058823529412\n"
     ]
    }
   ],
   "source": [
    "predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                        return_sites=(\"beta\", \"alpha\", \"mu_c\", \"sigma_c\"))\n",
    "samples = predictive(X_train, n_cat, n_acc, n_light, n_road, 12, 10, 10, y_train-1)\n",
    "\n",
    "alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()\n",
    "\n",
    "acc = X_test[:,0]\n",
    "light = X_test[:,1]\n",
    "road =  X_test[:,2]\n",
    "y_hat = alpha_hat[acc-1,light-1,road-1,:] + np.squeeze(np.dot(X_test, beta_hat[0]))\n",
    "y_hat = np.argmax(y_hat, axis=1) + 1\n",
    "print(\"predictions:\", y_hat)\n",
    "print(\"true values:\", y_test)\n",
    "\n",
    "# evaluate prediction accuracy\n",
    "print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a08f3",
   "metadata": {},
   "source": [
    "Here we improve our accuracy by 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d5428",
   "metadata": {},
   "source": [
    "The next thing we tried was to use individual priors for the mean and sd. for each of the categories. Here we due to long computation time tuned the parameters by for each element in each prior, incrementing the given element by one, computing the accuracy. If incrementing the element increased the accuracy we keep it and if not we stopped tuning this element and moved to tuning the next element in the prior vector. We start by assuming that they are = [1,1,1,1], we then increment each element one at a time and if it doesn't increasethe accuracy with more than 0.5% we move on. \n",
    "\n",
    "The drawback with this method is that we don't test all combinations of value choices, but due to long computation times we assessed that this would be too time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "7da73f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for l= tensor([1, 1, 1, 1]) t= 10 nu= 7 acc =  0.5694117647058824\n",
      "acc for l= tensor([2, 1, 1, 1]) t= 10 nu= 7 acc =  0.5676470588235294\n",
      "acc for l= tensor([3, 1, 1, 1]) t= 10 nu= 7 acc =  0.5705882352941176\n",
      "acc for l= tensor([4, 1, 1, 1]) t= 10 nu= 7 acc =  0.5752941176470588\n",
      "acc for l= tensor([5, 1, 1, 1]) t= 10 nu= 7 acc =  0.5794117647058824\n",
      "acc for l= tensor([6, 1, 1, 1]) t= 10 nu= 7 acc =  0.5723529411764706\n",
      "acc for l= tensor([5, 2, 1, 1]) t= 10 nu= 7 acc =  0.5711764705882353\n",
      "acc for l= tensor([5, 1, 2, 1]) t= 10 nu= 7 acc =  0.5711764705882353\n",
      "acc for l= tensor([5, 1, 1, 2]) t= 10 nu= 7 acc =  0.5776470588235294\n",
      "acc for l= tensor([5, 1, 1, 3]) t= 10 nu= 7 acc =  0.5705882352941176\n"
     ]
    }
   ],
   "source": [
    "lambda_val = torch.tensor([1,1,1,1])\n",
    "tau_val = 10\n",
    "nu_val = 7\n",
    "\n",
    "n_steps = 100\n",
    "acc_mat = np.zeros(n_steps - 1)\n",
    "\n",
    "j = 0\n",
    "acc_mat[0] = 0\n",
    "for i in range(1,n_steps):\n",
    "    \n",
    "    # Define guide function\n",
    "    guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "    # Reset parameter values\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Define the number of optimization steps\n",
    "    n_steps = 2500\n",
    "\n",
    "    # Setup the optimizer\n",
    "    adam_params = {\"lr\": 0.005}\n",
    "    optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "    # Setup the inference algorithm\n",
    "    elbo = Trace_ELBO(num_particles=3)\n",
    "    svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu_val, y_train-1)\n",
    "\n",
    "    predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                            return_sites=(\"beta\", \"alpha\", \"alpha_mu\", \"alpha_sigma\"))\n",
    "    samples = predictive(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau_val, nu_val, y_train-1)\n",
    "    # extract expected values of the parameters\n",
    "    alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "    beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()\n",
    "\n",
    "    # make predictions for test set\n",
    "    acc = X_test[:,0]\n",
    "    light = X_test[:,1]\n",
    "    road =  X_test[:,2]\n",
    "    y_hat = alpha_hat[acc-1,light-1,road-1,:] + np.squeeze(np.dot(X_test, beta_hat[0]))\n",
    "    y_hat = np.argmax(y_hat, axis=1) + 1\n",
    "\n",
    "    # evaluate prediction accuracy\n",
    "    acc_mat[i] = 1.0*np.sum(y_hat == y_test) / len(y_test)\n",
    "    print(\"acc for l=\",lambda_val, \"t=\",tau_val, \"nu=\",nu_val, \"acc = \", acc_mat[i])\n",
    "    \n",
    "    if (acc_mat[i-1] - 0.005 > acc_mat[i]) & (j < 3):\n",
    "        lambda_val[j] = lambda_val[j] - 1\n",
    "        acc_mat[i] = acc_mat[i-1]\n",
    "        j += 1\n",
    "    if (acc_mat[i-1] - 0.005 > acc_mat[i]) & (j == 3):\n",
    "        break\n",
    "    \n",
    "    lambda_val[j] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "550b1cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([1, 1, 1, 1]) nu= 7 acc =  0.5670588235294117\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([2, 1, 1, 1]) nu= 7 acc =  0.5752941176470588\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 1, 1, 1]) nu= 7 acc =  0.5758823529411765\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([4, 1, 1, 1]) nu= 7 acc =  0.5682352941176471\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 2, 1, 1]) nu= 7 acc =  0.5788235294117647\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 3, 1, 1]) nu= 7 acc =  0.5741176470588235\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 4, 1, 1]) nu= 7 acc =  0.571764705882353\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 5, 1, 1]) nu= 7 acc =  0.5770588235294117\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= 7 acc =  0.58\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 7, 1, 1]) nu= 7 acc =  0.5723529411764706\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 2, 1]) nu= 7 acc =  0.5729411764705883\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 2]) nu= 7 acc =  0.5764705882352941\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 3]) nu= 7 acc =  0.5782352941176471\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 4]) nu= 7 acc =  0.5676470588235294\n"
     ]
    }
   ],
   "source": [
    "lambda_val = torch.tensor([5, 1, 1, 1])\n",
    "tau_val = torch.tensor([1,1,1,1])\n",
    "nu_val = 7\n",
    "\n",
    "n_steps = 100\n",
    "acc_mat = np.zeros(n_steps - 1)\n",
    "\n",
    "j = 0\n",
    "acc_mat[0] = 0\n",
    "for i in range(1,n_steps):\n",
    "    \n",
    "    # Define guide function\n",
    "    guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "    # Reset parameter values\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Define the number of optimization steps\n",
    "    n_steps = 2500\n",
    "\n",
    "    # Setup the optimizer\n",
    "    adam_params = {\"lr\": 0.005}\n",
    "    optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "    # Setup the inference algorithm\n",
    "    elbo = Trace_ELBO(num_particles=3)\n",
    "    svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu_val, y_train-1)\n",
    "\n",
    "    predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                            return_sites=(\"beta\", \"alpha\", \"alpha_mu\", \"alpha_sigma\"))\n",
    "    samples = predictive(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau_val, nu_val, y_train-1)\n",
    "    # extract expected values of the parameters\n",
    "    alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "    beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()\n",
    "\n",
    "    # make predictions for test set\n",
    "    acc = X_test[:,0]\n",
    "    light = X_test[:,1]\n",
    "    road =  X_test[:,2]\n",
    "    y_hat = alpha_hat[acc-1,light-1,road-1,:] + np.squeeze(np.dot(X_test, beta_hat[0]))\n",
    "    y_hat = np.argmax(y_hat, axis=1) + 1\n",
    "\n",
    "    # evaluate prediction accuracy\n",
    "    acc_mat[i] = 1.0*np.sum(y_hat == y_test) / len(y_test)\n",
    "    print(\"acc for l=\",lambda_val, \"t=\",tau_val, \"nu=\",nu_val, \"acc = \", acc_mat[i])\n",
    "    \n",
    "    if (acc_mat[i-1] - 0.005 > acc_mat[i]) & (j < 3):\n",
    "        tau_val[j] = tau_val[j] - 1\n",
    "        acc_mat[i] = acc_mat[i-1]\n",
    "        j += 1\n",
    "    if (acc_mat[i-1] - 0.005 > acc_mat[i]) & (j == 3):\n",
    "        break\n",
    "    \n",
    "    tau_val[j] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "6d7a5348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= tensor([1, 1, 1, 1]) acc =  0.5641176470588235\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= tensor([2, 1, 1, 1]) acc =  0.571764705882353\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= tensor([3, 1, 1, 1]) acc =  0.5670588235294117\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= tensor([4, 1, 1, 1]) acc =  0.5758823529411765\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= tensor([5, 1, 1, 1]) acc =  0.5682352941176471\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= tensor([4, 2, 1, 1]) acc =  0.5705882352941176\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= tensor([4, 1, 2, 1]) acc =  0.5664705882352942\n",
      "acc for l= tensor([5, 1, 1, 1]) t= tensor([3, 6, 1, 1]) nu= tensor([4, 1, 1, 2]) acc =  0.5670588235294117\n"
     ]
    }
   ],
   "source": [
    "lambda_val = torch.tensor([5, 1, 1, 1])\n",
    "tau_val = torch.tensor([3, 6, 1, 1])\n",
    "nu_val = torch.tensor([1,1,1,1])\n",
    "\n",
    "n_steps = 100\n",
    "acc_mat = np.zeros(n_steps - 1)\n",
    "\n",
    "j = 0\n",
    "acc_mat[0] = 0\n",
    "for i in range(1,n_steps):\n",
    "    \n",
    "    # Define guide function\n",
    "    guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "    # Reset parameter values\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Define the number of optimization steps\n",
    "    n_steps = 2500\n",
    "\n",
    "    # Setup the optimizer\n",
    "    adam_params = {\"lr\": 0.005}\n",
    "    optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "    # Setup the inference algorithm\n",
    "    elbo = Trace_ELBO(num_particles=3)\n",
    "    svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu_val, y_train-1)\n",
    "\n",
    "    predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                            return_sites=(\"beta\", \"alpha\", \"alpha_mu\", \"alpha_sigma\"))\n",
    "    samples = predictive(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau_val, nu_val, y_train-1)\n",
    "    # extract expected values of the parameters\n",
    "    alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "    beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()\n",
    "\n",
    "    # make predictions for test set\n",
    "    acc = X_test[:,0]\n",
    "    light = X_test[:,1]\n",
    "    road =  X_test[:,2]\n",
    "    y_hat = alpha_hat[acc-1,light-1,road-1,:] + np.squeeze(np.dot(X_test, beta_hat[0]))\n",
    "    y_hat = np.argmax(y_hat, axis=1) + 1\n",
    "\n",
    "    # evaluate prediction accuracy\n",
    "    acc_mat[i] = 1.0*np.sum(y_hat == y_test) / len(y_test)\n",
    "    print(\"acc for l=\",lambda_val, \"t=\",tau_val, \"nu=\",nu_val, \"acc = \", acc_mat[i])\n",
    "    \n",
    "    if (acc_mat[i-1] - 0.005 > acc_mat[i]) & (j < 3):\n",
    "        nu_val[j] = nu_val[j] - 1\n",
    "        acc_mat[i] = acc_mat[i-1]\n",
    "        j += 1\n",
    "    if (acc_mat[i-1] - 0.005 > acc_mat[i]) & (j == 3):\n",
    "        break\n",
    "    \n",
    "    nu_val[j] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f812b04",
   "metadata": {},
   "source": [
    "Here the maximum accuracy is found when nu = [4, 1, 1, 1], however we get an even higher accuracy when nu is 1-dimational i.e. for lambda = [5, 1, 1, 1], tau = [3, 6, 1, 1] and nu = 7  we get an accuracy of 58%, this is therefore our optimal choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0b365",
   "metadata": {},
   "source": [
    "# Extending the model with FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257085d1",
   "metadata": {},
   "source": [
    "Now we try to use a feed forward neural network in our hierarchical model. We wish to use this FFNN on the  $X\\beta$ part of the funtion that we use in the categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "92cd8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Architecture\n",
    "        self.in_layer = torch.nn.Linear(n_in, n_hidden)\n",
    "        self.h_layer = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.out_layer = torch.nn.Linear(n_hidden, n_out)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        X = self.tanh(self.in_layer(X))\n",
    "        X = self.tanh(self.h_layer(X))\n",
    "        X = self.out_layer(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "0b0fd8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_model(X, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu, obs=None):\n",
    "    torch_model = FFNN(n_in=3, n_hidden=3, n_out=n_cat)\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    mu_c = pyro.sample(\"mu_c\", dist.Normal(torch.zeros(n_cat), \n",
    "                                                       lambda_val*torch.ones(n_cat)).to_event())# Prior for the bias mean\n",
    "        \n",
    "\n",
    "    sigma_c  = pyro.sample(\"sigma_c\",  dist.HalfCauchy(tau*torch.ones(n_cat)).to_event()) # Prior for the bias standard deviation\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(input_dim, n_cat), \n",
    "                                            nu*torch.ones(input_dim, n_cat)).to_event()) # Priors for the regression coefficents\n",
    "   \n",
    "    bayesian_model =pyro.random_module('bayesian_model', torch_model, [mu_c,sigma_c,beta]) # Make this model and these priors a Pyro model\n",
    "    sampled_model = bayesian_model() \n",
    "    \n",
    "    with pyro.plate(\"acc\", n_acc):\n",
    "        with pyro.plate(\"light\", n_light):\n",
    "            with pyro.plate(\"road\", n_road):\n",
    "                alpha = pyro.sample(\"alpha\", dist.Normal(mu_c, sigma_c).to_event(1)) # Draw the individual parameter for each individual\n",
    "\n",
    "    with pyro.plate(\"data\", X.shape[0]):\n",
    "        acc = np.array(X[:,0])\n",
    "        light = np.array(X[:,1])\n",
    "        road = np.array(X[:,2])\n",
    "        nn1 = sampled_model(X.matmul(beta)).squeeze(-1)\n",
    "        logits = alpha[acc-1,light-1,road-1] + nn1 \n",
    "        y = pyro.sample(\"y\", dist.Categorical(logits=logits), obs=obs) # If you use logits you don't need to do sigmoid\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e21055",
   "metadata": {},
   "source": [
    "Because there are very few observaions in the 4'th category we now assume that there are only 3 categories. When doing this we saw that the model with FFNN performed better than with 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "aa411ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Pyro model with priors for each category\n",
    "tau = 10\n",
    "lambda_val= 12\n",
    "nu= 10\n",
    "n_cat = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "e8b3c779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frida/opt/anaconda3/lib/python3.8/site-packages/pyro/primitives.py:491: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 15040.0\n",
      "[500] ELBO: 4708.3\n",
      "[1000] ELBO: 3093.0\n",
      "[1500] ELBO: 2873.0\n",
      "[2000] ELBO: 2833.1\n",
      "[2500] ELBO: 2795.4\n",
      "[3000] ELBO: 2773.3\n",
      "[3500] ELBO: 2761.4\n",
      "[4000] ELBO: 2749.4\n",
      "[4500] ELBO: 2729.0\n",
      "[5000] ELBO: 2726.9\n",
      "[5500] ELBO: 2718.7\n",
      "[6000] ELBO: 2702.1\n",
      "[6500] ELBO: 2690.5\n",
      "[7000] ELBO: 2692.1\n",
      "[7500] ELBO: 2681.2\n",
      "[8000] ELBO: 2690.7\n",
      "[8500] ELBO: 2680.0\n",
      "[9000] ELBO: 2685.2\n",
      "[9500] ELBO: 2669.1\n",
      "[10000] ELBO: 2666.1\n",
      "[10500] ELBO: 2673.4\n",
      "[11000] ELBO: 2666.2\n",
      "[11500] ELBO: 2665.4\n",
      "CPU times: user 4min 11s, sys: 5.44 s, total: 4min 16s\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Train the herarical model with ANN\n",
    "\n",
    "\n",
    "# Define guide function\n",
    "guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 12000\n",
    "\n",
    "# Setup the optimizer\n",
    "adam_params = {\"lr\": 0.005}\n",
    "optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=3)\n",
    "svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(X_train, n_cat,n_acc, n_light, n_road, lambda_val, tau, ny, y_train-1)\n",
    "    if step % 500 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "49df36ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [2 2 2 ... 2 2 2]\n",
      "true values: [3 2 2 ... 2 2 3]\n",
      "Accuracy: 0.42058823529411765\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                        return_sites=(\"beta\", \"alpha\", \"mu_c\", \"sigma_c\"))\n",
    "samples = predictive(X_train, n_cat,n_acc, n_light, n_road, lambda_val, tau, nu, y_train-1)\n",
    "\n",
    "\n",
    "# extract expected values of the parameters\n",
    "alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()\n",
    "\n",
    "torch_model = FFNN(n_in=3, n_hidden=3, n_out=n_cat)\n",
    "tX_test = torch.tensor(X_test).float()\n",
    "# make predictions for test set\n",
    "acc = X_test[:,0]\n",
    "light = X_test[:,1]\n",
    "road =  X_test[:,2]\n",
    "yn = alpha_hat[acc-1,light-1,road-1,:]  + np.squeeze(torch_model(tX_test.matmul(torch.tensor(beta_hat[0]).float())).detach().numpy())\n",
    "yn = np.argmax(yn, axis=1) + 1\n",
    "print(\"predictions:\", yn)\n",
    "print(\"true values:\", y_test)\n",
    "\n",
    "# evaluate prediction accuracy\n",
    "print(\"Accuracy:\", 1.0*np.sum(1.0*np.sum(yn == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f17292",
   "metadata": {},
   "source": [
    "We here obtain an accuracy of 42%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b4d55",
   "metadata": {},
   "source": [
    "We now tune the priors like previously, the ranges chosen are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "7531f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for l= 8 t= 4 nu = 11 acc =  0.42058823529411765\n",
      "acc for l= 8 t= 4 nu = 12 acc =  0.42058823529411765\n",
      "acc for l= 8 t= 4 nu = 13 acc =  0.4676470588235294\n",
      "acc for l= 8 t= 4 nu = 14 acc =  0.03411764705882353\n",
      "acc for l= 8 t= 5 nu = 11 acc =  0.5452941176470588\n",
      "acc for l= 8 t= 5 nu = 12 acc =  0.03411764705882353\n",
      "acc for l= 8 t= 5 nu = 13 acc =  0.42058823529411765\n",
      "acc for l= 8 t= 5 nu = 14 acc =  0.31941176470588234\n",
      "acc for l= 8 t= 6 nu = 11 acc =  0.5452941176470588\n",
      "acc for l= 8 t= 6 nu = 12 acc =  0.03411764705882353\n",
      "acc for l= 8 t= 6 nu = 13 acc =  0.03411764705882353\n",
      "acc for l= 8 t= 6 nu = 14 acc =  0.03411764705882353\n",
      "acc for l= 8 t= 7 nu = 11 acc =  0.36411764705882355\n",
      "acc for l= 8 t= 7 nu = 12 acc =  0.03411764705882353\n",
      "acc for l= 8 t= 7 nu = 13 acc =  0.42058823529411765\n",
      "acc for l= 8 t= 7 nu = 14 acc =  0.42058823529411765\n",
      "acc for l= 9 t= 4 nu = 11 acc =  0.42058823529411765\n",
      "acc for l= 9 t= 4 nu = 12 acc =  0.1935294117647059\n",
      "acc for l= 9 t= 4 nu = 13 acc =  0.4676470588235294\n",
      "acc for l= 9 t= 4 nu = 14 acc =  0.07823529411764706\n",
      "acc for l= 9 t= 5 nu = 11 acc =  0.42058823529411765\n",
      "acc for l= 9 t= 5 nu = 12 acc =  0.42058823529411765\n",
      "acc for l= 9 t= 5 nu = 13 acc =  0.3058823529411765\n",
      "acc for l= 9 t= 5 nu = 14 acc =  0.5529411764705883\n",
      "acc for l= 9 t= 6 nu = 11 acc =  0.03411764705882353\n",
      "acc for l= 9 t= 6 nu = 12 acc =  0.03411764705882353\n",
      "acc for l= 9 t= 6 nu = 13 acc =  0.06\n",
      "acc for l= 9 t= 6 nu = 14 acc =  0.03411764705882353\n",
      "acc for l= 9 t= 7 nu = 11 acc =  0.37823529411764706\n",
      "acc for l= 9 t= 7 nu = 12 acc =  0.21588235294117647\n",
      "acc for l= 9 t= 7 nu = 13 acc =  0.03411764705882353\n",
      "acc for l= 9 t= 7 nu = 14 acc =  0.5452941176470588\n",
      "acc for l= 10 t= 4 nu = 11 acc =  0.4511764705882353\n",
      "acc for l= 10 t= 4 nu = 12 acc =  0.42058823529411765\n",
      "acc for l= 10 t= 4 nu = 13 acc =  0.42411764705882354\n",
      "acc for l= 10 t= 4 nu = 14 acc =  0.5452941176470588\n",
      "acc for l= 10 t= 5 nu = 11 acc =  0.5452941176470588\n",
      "acc for l= 10 t= 5 nu = 12 acc =  0.42058823529411765\n",
      "acc for l= 10 t= 5 nu = 13 acc =  0.03411764705882353\n",
      "acc for l= 10 t= 5 nu = 14 acc =  0.40058823529411763\n",
      "acc for l= 10 t= 6 nu = 11 acc =  0.42058823529411765\n",
      "acc for l= 10 t= 6 nu = 12 acc =  0.5629411764705883\n",
      "acc for l= 10 t= 6 nu = 13 acc =  0.03411764705882353\n",
      "acc for l= 10 t= 6 nu = 14 acc =  0.42411764705882354\n",
      "acc for l= 10 t= 7 nu = 11 acc =  0.41294117647058826\n",
      "acc for l= 10 t= 7 nu = 12 acc =  0.42058823529411765\n",
      "acc for l= 10 t= 7 nu = 13 acc =  0.03411764705882353\n",
      "acc for l= 10 t= 7 nu = 14 acc =  0.4176470588235294\n",
      "acc for l= 11 t= 4 nu = 11 acc =  0.38\n",
      "acc for l= 11 t= 4 nu = 12 acc =  0.03411764705882353\n",
      "acc for l= 11 t= 4 nu = 13 acc =  0.16705882352941176\n",
      "acc for l= 11 t= 4 nu = 14 acc =  0.5452941176470588\n",
      "acc for l= 11 t= 5 nu = 11 acc =  0.42058823529411765\n",
      "acc for l= 11 t= 5 nu = 12 acc =  0.03411764705882353\n",
      "acc for l= 11 t= 5 nu = 13 acc =  0.5452941176470588\n",
      "acc for l= 11 t= 5 nu = 14 acc =  0.051176470588235295\n",
      "acc for l= 11 t= 6 nu = 11 acc =  0.42058823529411765\n",
      "acc for l= 11 t= 6 nu = 12 acc =  0.42058823529411765\n",
      "acc for l= 11 t= 6 nu = 13 acc =  0.42058823529411765\n",
      "acc for l= 11 t= 6 nu = 14 acc =  0.42058823529411765\n",
      "acc for l= 11 t= 7 nu = 11 acc =  0.42411764705882354\n",
      "acc for l= 11 t= 7 nu = 12 acc =  0.42058823529411765\n",
      "acc for l= 11 t= 7 nu = 13 acc =  0.4641176470588235\n",
      "acc for l= 11 t= 7 nu = 14 acc =  0.42058823529411765\n",
      "acc for l= 12 t= 4 nu = 11 acc =  0.42058823529411765\n",
      "acc for l= 12 t= 4 nu = 12 acc =  0.03411764705882353\n",
      "acc for l= 12 t= 4 nu = 13 acc =  0.5452941176470588\n",
      "acc for l= 12 t= 4 nu = 14 acc =  0.03411764705882353\n",
      "acc for l= 12 t= 5 nu = 11 acc =  0.49411764705882355\n",
      "acc for l= 12 t= 5 nu = 12 acc =  0.4429411764705882\n",
      "acc for l= 12 t= 5 nu = 13 acc =  0.03411764705882353\n",
      "acc for l= 12 t= 5 nu = 14 acc =  0.42058823529411765\n",
      "acc for l= 12 t= 6 nu = 11 acc =  0.5452941176470588\n",
      "acc for l= 12 t= 6 nu = 12 acc =  0.03411764705882353\n",
      "acc for l= 12 t= 6 nu = 13 acc =  0.03411764705882353\n",
      "acc for l= 12 t= 6 nu = 14 acc =  0.19294117647058823\n",
      "acc for l= 12 t= 7 nu = 11 acc =  0.42058823529411765\n",
      "acc for l= 12 t= 7 nu = 12 acc =  0.5452941176470588\n",
      "acc for l= 12 t= 7 nu = 13 acc =  0.03411764705882353\n",
      "acc for l= 12 t= 7 nu = 14 acc =  0.42058823529411765\n"
     ]
    }
   ],
   "source": [
    "# Ranges\n",
    "l_v = [8, 9, 10, 11, 12]\n",
    "t_v = [4, 5, 6, 7]\n",
    "n_c = [11, 12, 13, 14]\n",
    "\n",
    "acc_mat = np.zeros((len(l_v),len(t_v),len(n_c)))\n",
    "l=0\n",
    "for lambda_val in l_v:\n",
    "    t = 0\n",
    "    for tau in t_v:\n",
    "        b=0\n",
    "        for nu in n_c:\n",
    "            \n",
    "            # Define guide function\n",
    "            guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "            # Reset parameter values\n",
    "            pyro.clear_param_store()\n",
    "\n",
    "            # Define the number of optimization steps\n",
    "            n_steps = 2500\n",
    "\n",
    "            # Setup the optimizer\n",
    "            adam_params = {\"lr\": 0.005}\n",
    "            optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "            # Setup the inference algorithm\n",
    "            elbo = Trace_ELBO(num_particles=3)\n",
    "            svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "            # Do gradient steps\n",
    "            for step in range(n_steps):\n",
    "                elbo = svi.step(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu, y_train-1)\n",
    "                \n",
    "            predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                                    return_sites=(\"beta\", \"alpha\", \"alpha_mu\", \"alpha_sigma\"))\n",
    "            samples = predictive(X_train, n_cat, n_acc, n_light, n_road, lambda_val, tau, nu, y_train-1)\n",
    "            \n",
    "            # extract expected values of the parameters\n",
    "            alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "            beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()\n",
    "\n",
    "            torch_model = FFNN(n_in=3, n_hidden=3, n_out=n_cat)\n",
    "            tX_test = torch.tensor(X_test).float()\n",
    "            # make predictions for test set\n",
    "            acc = X_test[:,0]\n",
    "            light = X_test[:,1]\n",
    "            road =  X_test[:,2]\n",
    "            yn = alpha_hat[acc-1,light-1,road-1,:]  + np.squeeze(torch_model(tX_test.matmul(torch.tensor(beta_hat[0]).float())).detach().numpy())\n",
    "            yn = np.argmax(yn, axis=1) + 1\n",
    "\n",
    "            # evaluate prediction accuracy\n",
    "            acc_mat[l,t,b] = 1.0*np.sum(1.0*np.sum(yn == y_test) / len(y_test))\n",
    "            print(\"acc for l=\",lambda_val, \"t=\",tau, \"nu =\",nu, \"acc = \", acc_mat[l,t,b])\n",
    "            \n",
    "            b+=1\n",
    "        t+=1\n",
    "    l+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "3ab79cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5629411764705883"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(acc_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a343358",
   "metadata": {},
   "source": [
    "We then train the model with lambda = 10 tau = 6 nu = 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "6160a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 15618.1\n",
      "[500] ELBO: 3343.5\n",
      "[1000] ELBO: 2767.9\n",
      "[1500] ELBO: 2728.4\n",
      "[2000] ELBO: 2710.0\n",
      "CPU times: user 50.9 s, sys: 807 ms, total: 51.7 s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Train the herarical model with ANN\n",
    "\n",
    "\n",
    "# Define guide function\n",
    "guide = AutoDiagonalNormal(hierarchical_model)\n",
    "\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 2500\n",
    "\n",
    "# Setup the optimizer\n",
    "adam_params = {\"lr\": 0.005}\n",
    "optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=3)\n",
    "svi = SVI(hierarchical_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(X_train, n_cat,n_acc, n_light, n_road, 10, 6, 12, y_train-1)\n",
    "    if step % 500 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "3cf0c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [3 3 3 ... 3 3 3]\n",
      "true values: [3 2 2 ... 2 2 3]\n",
      "Accuracy: 0.5452941176470588\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(hierarchical_model, guide=guide, num_samples=2000,\n",
    "                        return_sites=(\"beta\", \"alpha\", \"mu_c\", \"sigma_c\"))\n",
    "samples = predictive(X_train, n_cat,n_acc, n_light, n_road, 10, 6, 12, y_train-1)\n",
    "\n",
    "\n",
    "# extract expected values of the parameters\n",
    "alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n",
    "beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()\n",
    "\n",
    "torch_model = FFNN(n_in=3, n_hidden=3, n_out=n_cat)\n",
    "tX_test = torch.tensor(X_test).float()\n",
    "# make predictions for test set\n",
    "acc = X_test[:,0]\n",
    "light = X_test[:,1]\n",
    "road =  X_test[:,2]\n",
    "yn = alpha_hat[acc-1,light-1,road-1,:]  + np.squeeze(torch_model(tX_test.matmul(torch.tensor(beta_hat[0]).float())).detach().numpy())\n",
    "yn = np.argmax(yn, axis=1) + 1\n",
    "print(\"predictions:\", yn)\n",
    "print(\"true values:\", y_test)\n",
    "\n",
    "# evaluate prediction accuracy\n",
    "print(\"Accuracy:\", 1.0*np.sum(1.0*np.sum(yn == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d61f9c",
   "metadata": {},
   "source": [
    "Here we obtain an accuracy of 54.5 %"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
